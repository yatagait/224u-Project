Namespace(datasource='amazonreview', select_data=-1, test_dataset=-1, num_classes=5, num_test_task=600, test_epoch=-1, metatrain_iterations=15000, meta_batch_size=1, update_lr=0.001, meta_lr=2e-05, num_updates=5, num_updates_test=10, update_batch_size=1, update_batch_size_eval=5, num_filters=64, weight_decay=0.0, log=1, logdir='xxx', datadir='/iris/u/huaxiu/Data', resume=0, train=1, test_set=1, use_kg=0, trail=0, warm_epoch=0, ratio=1.0)
ProtoNet.data_amazonreview.cls_5.mbs_1.ubs_1.metalr2e-05.innerlr0.001.numupdates5
Downloading:   0%|          | 0.00/684 [00:00<?, ?B/s]Downloading: 100%|██████████| 684/684 [00:00<00:00, 400kB/s]
Downloading:   0%|          | 0.00/47.4M [00:00<?, ?B/s]Downloading:   8%|▊         | 3.97M/47.4M [00:00<00:01, 39.7MB/s]Downloading:  28%|██▊       | 13.1M/47.4M [00:00<00:00, 70.1MB/s]Downloading:  49%|████▉     | 23.3M/47.4M [00:00<00:00, 84.7MB/s]Downloading:  72%|███████▏  | 34.0M/47.4M [00:00<00:00, 93.6MB/s]Downloading:  96%|█████████▌| 45.3M/47.4M [00:00<00:00, 100MB/s] Downloading: 100%|██████████| 47.4M/47.4M [00:00<00:00, 91.2MB/s]
Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]Downloading:   9%|▉         | 70.7k/760k [00:00<00:01, 527kB/s]Downloading:  39%|███▉      | 300k/760k [00:00<00:00, 1.21MB/s]Downloading: 100%|██████████| 760k/760k [00:00<00:00, 2.21MB/s]
Downloading:   0%|          | 0.00/1.31M [00:00<?, ?B/s]Downloading:   6%|▌         | 77.8k/1.31M [00:00<00:02, 571kB/s]Downloading:  23%|██▎       | 307k/1.31M [00:00<00:00, 1.22MB/s]Downloading:  86%|████████▌ | 1.13M/1.31M [00:00<00:00, 3.32MB/s]Downloading: 100%|██████████| 1.31M/1.31M [00:00<00:00, 3.10MB/s]
iter: 100, loss_all: 1.3728946447372437, acc: 0.4328000843524933
iter: 200, loss_all: 0.8611061573028564, acc: 0.6612000465393066
iter: 300, loss_all: 0.7739648818969727, acc: 0.7076001763343811
iter: 400, loss_all: 0.656704843044281, acc: 0.7440000772476196
iter: 500, loss_all: 0.6464059352874756, acc: 0.7544000744819641
iter: 600, loss_all: 0.5410516858100891, acc: 0.7879998683929443
iter: 700, loss_all: 0.4199800491333008, acc: 0.8339999318122864
iter: 800, loss_all: 0.4816492795944214, acc: 0.8176000118255615
iter: 900, loss_all: 0.3812148869037628, acc: 0.8504002690315247
iter: 1000, loss_all: 0.34186050295829773, acc: 0.8651999831199646
iter: 1100, loss_all: 0.2946568727493286, acc: 0.8883997797966003
iter: 1200, loss_all: 0.2624276578426361, acc: 0.9039998054504395
iter: 1300, loss_all: 0.2614009976387024, acc: 0.9007997512817383
iter: 1400, loss_all: 0.23351562023162842, acc: 0.9151996970176697
iter: 1500, loss_all: 0.20191740989685059, acc: 0.9171996116638184
iter: 1600, loss_all: 0.1820286065340042, acc: 0.9319995045661926
iter: 1700, loss_all: 0.1632552444934845, acc: 0.9315997362136841
iter: 1800, loss_all: 0.1481267809867859, acc: 0.9367995858192444
iter: 1900, loss_all: 0.13889916241168976, acc: 0.9459993839263916
iter: 2000, loss_all: 0.13921436667442322, acc: 0.9427993297576904
iter: 2100, loss_all: 0.11889635771512985, acc: 0.9515994787216187
iter: 2200, loss_all: 0.10161338746547699, acc: 0.9595995545387268
iter: 2300, loss_all: 0.08629105240106583, acc: 0.96519935131073
iter: 2400, loss_all: 0.057123374193906784, acc: 0.9699994325637817
iter: 2500, loss_all: 0.06366132199764252, acc: 0.9687994122505188
iter: 2600, loss_all: 0.06206472963094711, acc: 0.968799352645874
iter: 2700, loss_all: 0.05876298248767853, acc: 0.9679993987083435
iter: 2800, loss_all: 0.035147275775671005, acc: 0.9803994297981262
iter: 2900, loss_all: 0.0633496642112732, acc: 0.96519935131073
iter: 3000, loss_all: 0.07397796958684921, acc: 0.9663993716239929
iter: 3100, loss_all: 0.06372914463281631, acc: 0.9695994257926941
iter: 3200, loss_all: 0.06182971969246864, acc: 0.9711994528770447
iter: 3300, loss_all: 0.046271517872810364, acc: 0.9707993268966675
iter: 3400, loss_all: 0.03769627586007118, acc: 0.9763994812965393
iter: 3500, loss_all: 0.06767147034406662, acc: 0.9687994718551636
iter: 3600, loss_all: 0.04532431438565254, acc: 0.9743993878364563
iter: 3700, loss_all: 0.021391797810792923, acc: 0.9811994433403015
iter: 3800, loss_all: 0.03299938514828682, acc: 0.9795994162559509
iter: 3900, loss_all: 0.05708689242601395, acc: 0.9719993472099304
iter: 4000, loss_all: 0.03328084200620651, acc: 0.9775994420051575
iter: 4100, loss_all: 0.03195881098508835, acc: 0.9799994230270386
iter: 4200, loss_all: 0.025659343227744102, acc: 0.9835994839668274
iter: 4300, loss_all: 0.021330630406737328, acc: 0.9835993647575378
iter: 4400, loss_all: 0.019697511568665504, acc: 0.9847993850708008
iter: 4500, loss_all: 0.024555670097470284, acc: 0.981999397277832
iter: 4600, loss_all: 0.023531489074230194, acc: 0.983199417591095
iter: 4700, loss_all: 0.029895644634962082, acc: 0.9823994040489197
iter: 4800, loss_all: 0.04433520883321762, acc: 0.9779994487762451
iter: 4900, loss_all: 0.02841171622276306, acc: 0.9827993512153625
iter: 5000, loss_all: 0.014310111291706562, acc: 0.9851993918418884
iter: 5100, loss_all: 0.03493465483188629, acc: 0.9767993688583374
iter: 5200, loss_all: 0.025972727686166763, acc: 0.981999397277832
iter: 5300, loss_all: 0.008690538816154003, acc: 0.9871993660926819
iter: 5400, loss_all: 0.011402980424463749, acc: 0.9851994514465332
iter: 5500, loss_all: 0.016350332647562027, acc: 0.9827993512153625
iter: 5600, loss_all: 0.02661209926009178, acc: 0.9815994501113892
iter: 5700, loss_all: 0.014374603517353535, acc: 0.9859994053840637
iter: 5800, loss_all: 0.01040058396756649, acc: 0.9871993660926819
iter: 5900, loss_all: 0.00710765365511179, acc: 0.9883993864059448
iter: 6000, loss_all: 0.014849754050374031, acc: 0.983199417591095
iter: 6100, loss_all: 0.024551047012209892, acc: 0.9839992523193359
iter: 6200, loss_all: 0.04636574536561966, acc: 0.9767994284629822
iter: 6300, loss_all: 0.021767528727650642, acc: 0.983199417591095
iter: 6400, loss_all: 0.006228816229850054, acc: 0.9883993268013
iter: 6500, loss_all: 0.034138910472393036, acc: 0.9791994690895081
iter: 6600, loss_all: 0.02787814848124981, acc: 0.9807993769645691
iter: 6700, loss_all: 0.017515748739242554, acc: 0.983199417591095
iter: 6800, loss_all: 0.029521850869059563, acc: 0.979999303817749
iter: 6900, loss_all: 0.04695964977145195, acc: 0.9751994013786316
iter: 7000, loss_all: 0.025283928960561752, acc: 0.981999397277832
iter: 7100, loss_all: 0.027627456933259964, acc: 0.981199324131012
iter: 7200, loss_all: 0.04072777181863785, acc: 0.9759995341300964
iter: 7300, loss_all: 0.013620504178106785, acc: 0.9871994256973267
iter: 7400, loss_all: 0.009437133558094501, acc: 0.9867993593215942
iter: 7500, loss_all: 0.0035399580374360085, acc: 0.9883993864059448
iter: 7600, loss_all: 0.004271099343895912, acc: 0.9875993728637695
iter: 7700, loss_all: 0.004808865021914244, acc: 0.9883993268013
iter: 7800, loss_all: 0.038333285599946976, acc: 0.9791994690895081
iter: 7900, loss_all: 0.007633922155946493, acc: 0.9875993132591248
iter: 8000, loss_all: 0.01156863383948803, acc: 0.9863993525505066
iter: 8100, loss_all: 0.009960826486349106, acc: 0.9871993660926819
iter: 8200, loss_all: 0.027795717120170593, acc: 0.9799994230270386
iter: 8300, loss_all: 0.024801375344395638, acc: 0.9815995097160339
iter: 8400, loss_all: 0.010460734367370605, acc: 0.986799418926239
iter: 8500, loss_all: 0.012654603458940983, acc: 0.9871993660926819
iter: 8600, loss_all: 0.026965053752064705, acc: 0.9823993444442749
iter: 8700, loss_all: 0.01821606233716011, acc: 0.9831992983818054
iter: 8800, loss_all: 0.006180703639984131, acc: 0.9875993728637695
iter: 8900, loss_all: 0.01457714568823576, acc: 0.985999345779419
iter: 9000, loss_all: 0.010251459665596485, acc: 0.9851994514465332
iter: 9100, loss_all: 0.0135458679869771, acc: 0.9871994256973267
iter: 9200, loss_all: 0.01897074468433857, acc: 0.9843993186950684
iter: 9300, loss_all: 0.00723197590559721, acc: 0.9883993864059448
iter: 9400, loss_all: 0.007958689704537392, acc: 0.9879993796348572
iter: 9500, loss_all: 0.006562748458236456, acc: 0.9875994324684143
iter: 9600, loss_all: 0.018274055793881416, acc: 0.9839993715286255
iter: 9700, loss_all: 0.011970837600529194, acc: 0.9855993390083313
iter: 9800, loss_all: 0.008088096976280212, acc: 0.9855993390083313
iter: 9900, loss_all: 0.00735009741038084, acc: 0.9863994121551514
iter: 10000, loss_all: 0.012551629915833473, acc: 0.9863994121551514
iter: 10100, loss_all: 0.015408379025757313, acc: 0.9851993918418884
iter: 10200, loss_all: 0.022912288084626198, acc: 0.9799994230270386
iter: 10300, loss_all: 0.013507477007806301, acc: 0.9855993986129761
iter: 10400, loss_all: 0.019842777401208878, acc: 0.9827994108200073
iter: 10500, loss_all: 0.0195307657122612, acc: 0.9839993715286255
iter: 10600, loss_all: 0.020752638578414917, acc: 0.984799325466156
iter: 10700, loss_all: 0.00952699314802885, acc: 0.9871993660926819
iter: 10800, loss_all: 0.0018317240756005049, acc: 0.989599347114563
iter: 10900, loss_all: 0.018926281481981277, acc: 0.9851993918418884
iter: 11000, loss_all: 0.015530083328485489, acc: 0.9851993322372437
iter: 11100, loss_all: 0.013420864939689636, acc: 0.9871993660926819
iter: 11200, loss_all: 0.011844209395349026, acc: 0.9875993728637695
iter: 11300, loss_all: 0.01761474646627903, acc: 0.9835993051528931
iter: 11400, loss_all: 0.017360374331474304, acc: 0.9835994243621826
iter: 11500, loss_all: 0.010854074731469154, acc: 0.9867993593215942
iter: 11600, loss_all: 0.003952173516154289, acc: 0.9887993335723877
iter: 11700, loss_all: 0.021472731605172157, acc: 0.9843993782997131
iter: 11800, loss_all: 0.013971992768347263, acc: 0.9855993986129761
iter: 11900, loss_all: 0.006706533022224903, acc: 0.9875993728637695
iter: 12000, loss_all: 0.007970241829752922, acc: 0.9875993728637695
iter: 12100, loss_all: 0.012358387000858784, acc: 0.9851993322372437
iter: 12200, loss_all: 0.01243766862899065, acc: 0.9855993986129761
iter: 12300, loss_all: 0.018150299787521362, acc: 0.9855992794036865
iter: 12400, loss_all: 0.012004275806248188, acc: 0.9871993660926819
iter: 12500, loss_all: 0.010312143713235855, acc: 0.9871994256973267
iter: 12600, loss_all: 0.0036141234450042248, acc: 0.9891993403434753
iter: 12700, loss_all: 0.0239186380058527, acc: 0.9823994636535645
iter: 12800, loss_all: 0.01774352230131626, acc: 0.9843993186950684
iter: 12900, loss_all: 0.03539863973855972, acc: 0.9787994027137756
iter: 13000, loss_all: 0.003511232789605856, acc: 0.9887993931770325
iter: 13100, loss_all: 0.005799804348498583, acc: 0.9887993335723877
iter: 13200, loss_all: 0.002567485673353076, acc: 0.9883993864059448
iter: 13300, loss_all: 0.0011988759506493807, acc: 0.9899993538856506
iter: 13400, loss_all: 0.0035623309668153524, acc: 0.9879993796348572
iter: 13500, loss_all: 0.0044531445018947124, acc: 0.9883993268013
iter: 13600, loss_all: 0.018454374745488167, acc: 0.9835993051528931
iter: 13700, loss_all: 0.022602928802371025, acc: 0.9839993715286255
iter: 13800, loss_all: 0.018254343420267105, acc: 0.9835994243621826
iter: 13900, loss_all: 0.013302747160196304, acc: 0.9863993525505066
iter: 14000, loss_all: 0.005460849031805992, acc: 0.9879993200302124
iter: 14100, loss_all: 0.010656275786459446, acc: 0.9871994256973267
iter: 14200, loss_all: 0.016441598534584045, acc: 0.9847993850708008
iter: 14300, loss_all: 0.01255055982619524, acc: 0.9859994053840637
iter: 14400, loss_all: 0.006547634955495596, acc: 0.9887993335723877
iter: 14500, loss_all: 0.01093004085123539, acc: 0.9863994121551514
iter: 14600, loss_all: 0.012384063564240932, acc: 0.9851993918418884
iter: 14700, loss_all: 0.0057061221450567245, acc: 0.9883993864059448
iter: 14800, loss_all: 0.004675843752920628, acc: 0.9887993931770325
iter: 14900, loss_all: 0.004606923088431358, acc: 0.9887993931770325
iter: 15000, loss_all: 0.009088766761124134, acc: 0.986799418926239
Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Namespace(datasource='amazonreview', select_data=-1, test_dataset=-1, num_classes=5, num_test_task=600, test_epoch=-1, metatrain_iterations=15000, meta_batch_size=1, update_lr=0.001, meta_lr=2e-05, num_updates=5, num_updates_test=10, update_batch_size=1, update_batch_size_eval=5, num_filters=64, weight_decay=0.0, mix=False, log=1, logdir='xxx', datadir='/iris/u/huaxiu/Data', resume=0, train=1, test_set=1, use_kg=0, trail=0, warm_epoch=0, ratio=1.0, temp_scaling=1.0)
ProtoNet.data_amazonreview.cls_5.mbs_1.ubs_1.metalr2e-05.innerlr0.001.numupdates5
iter: 100, loss_all: 1.3728946447372437, acc: 0.4328000843524933
iter: 200, loss_all: 0.8611061573028564, acc: 0.6612000465393066
iter: 300, loss_all: 0.7739648818969727, acc: 0.7076001763343811
iter: 400, loss_all: 0.656704843044281, acc: 0.7440000772476196
iter: 500, loss_all: 0.6464059352874756, acc: 0.7544000744819641
iter: 600, loss_all: 0.5410516858100891, acc: 0.7879998683929443
iter: 700, loss_all: 0.4199800491333008, acc: 0.8339999318122864
iter: 800, loss_all: 0.4816492795944214, acc: 0.8176000118255615
iter: 900, loss_all: 0.3812148869037628, acc: 0.8504002690315247
iter: 1000, loss_all: 0.34186050295829773, acc: 0.8651999831199646
iter: 1100, loss_all: 0.2946568727493286, acc: 0.8883997797966003
iter: 1200, loss_all: 0.2624276578426361, acc: 0.9039998054504395
iter: 1300, loss_all: 0.2614009976387024, acc: 0.9007997512817383
iter: 1400, loss_all: 0.23351562023162842, acc: 0.9151996970176697
iter: 1500, loss_all: 0.20191740989685059, acc: 0.9171996116638184
iter: 1600, loss_all: 0.1820286065340042, acc: 0.9319995045661926
iter: 1700, loss_all: 0.1632552444934845, acc: 0.9315997362136841
iter: 1800, loss_all: 0.1481267809867859, acc: 0.9367995858192444
iter: 1900, loss_all: 0.13889916241168976, acc: 0.9459993839263916
iter: 2000, loss_all: 0.13921436667442322, acc: 0.9427993297576904
iter: 2100, loss_all: 0.11889635771512985, acc: 0.9515994787216187
iter: 2200, loss_all: 0.10161338746547699, acc: 0.9595995545387268
iter: 2300, loss_all: 0.08629105240106583, acc: 0.96519935131073
iter: 2400, loss_all: 0.057123374193906784, acc: 0.9699994325637817
iter: 2500, loss_all: 0.06366132199764252, acc: 0.9687994122505188
iter: 2600, loss_all: 0.06206472963094711, acc: 0.968799352645874
iter: 2700, loss_all: 0.05876298248767853, acc: 0.9679993987083435
iter: 2800, loss_all: 0.035147275775671005, acc: 0.9803994297981262
iter: 2900, loss_all: 0.0633496642112732, acc: 0.96519935131073
iter: 3000, loss_all: 0.07397796958684921, acc: 0.9663993716239929
iter: 3100, loss_all: 0.06372914463281631, acc: 0.9695994257926941
iter: 3200, loss_all: 0.06182971969246864, acc: 0.9711994528770447
iter: 3300, loss_all: 0.046271517872810364, acc: 0.9707993268966675
iter: 3400, loss_all: 0.03769627586007118, acc: 0.9763994812965393
iter: 3500, loss_all: 0.06767147034406662, acc: 0.9687994718551636
iter: 3600, loss_all: 0.04532431438565254, acc: 0.9743993878364563
iter: 3700, loss_all: 0.021391797810792923, acc: 0.9811994433403015
iter: 3800, loss_all: 0.03299938514828682, acc: 0.9795994162559509
iter: 3900, loss_all: 0.05708689242601395, acc: 0.9719993472099304
iter: 4000, loss_all: 0.03328084200620651, acc: 0.9775994420051575
iter: 4100, loss_all: 0.03195881098508835, acc: 0.9799994230270386
iter: 4200, loss_all: 0.025659343227744102, acc: 0.9835994839668274
iter: 4300, loss_all: 0.021330630406737328, acc: 0.9835993647575378
iter: 4400, loss_all: 0.019697511568665504, acc: 0.9847993850708008
iter: 4500, loss_all: 0.024555670097470284, acc: 0.981999397277832
iter: 4600, loss_all: 0.023531489074230194, acc: 0.983199417591095
iter: 4700, loss_all: 0.029895644634962082, acc: 0.9823994040489197
iter: 4800, loss_all: 0.04433520883321762, acc: 0.9779994487762451
iter: 4900, loss_all: 0.02841171622276306, acc: 0.9827993512153625
iter: 5000, loss_all: 0.014310111291706562, acc: 0.9851993918418884
iter: 5100, loss_all: 0.03493465483188629, acc: 0.9767993688583374
iter: 5200, loss_all: 0.025972727686166763, acc: 0.981999397277832
iter: 5300, loss_all: 0.008690538816154003, acc: 0.9871993660926819
iter: 5400, loss_all: 0.011402980424463749, acc: 0.9851994514465332
iter: 5500, loss_all: 0.016350332647562027, acc: 0.9827993512153625
iter: 5600, loss_all: 0.02661209926009178, acc: 0.9815994501113892
iter: 5700, loss_all: 0.014374603517353535, acc: 0.9859994053840637
iter: 5800, loss_all: 0.01040058396756649, acc: 0.9871993660926819
iter: 5900, loss_all: 0.00710765365511179, acc: 0.9883993864059448
iter: 6000, loss_all: 0.014849754050374031, acc: 0.983199417591095
iter: 6100, loss_all: 0.024551047012209892, acc: 0.9839992523193359
iter: 6200, loss_all: 0.04636574536561966, acc: 0.9767994284629822
iter: 6300, loss_all: 0.021767528727650642, acc: 0.983199417591095
iter: 6400, loss_all: 0.006228816229850054, acc: 0.9883993268013
iter: 6500, loss_all: 0.034138910472393036, acc: 0.9791994690895081
iter: 6600, loss_all: 0.02787814848124981, acc: 0.9807993769645691
iter: 6700, loss_all: 0.017515748739242554, acc: 0.983199417591095
iter: 6800, loss_all: 0.029521850869059563, acc: 0.979999303817749
iter: 6900, loss_all: 0.04695964977145195, acc: 0.9751994013786316
iter: 7000, loss_all: 0.025283928960561752, acc: 0.981999397277832
iter: 7100, loss_all: 0.027627456933259964, acc: 0.981199324131012
iter: 7200, loss_all: 0.04072777181863785, acc: 0.9759995341300964
iter: 7300, loss_all: 0.013620504178106785, acc: 0.9871994256973267
iter: 7400, loss_all: 0.009437133558094501, acc: 0.9867993593215942
iter: 7500, loss_all: 0.0035399580374360085, acc: 0.9883993864059448
iter: 7600, loss_all: 0.004271099343895912, acc: 0.9875993728637695
iter: 7700, loss_all: 0.004808865021914244, acc: 0.9883993268013
iter: 7800, loss_all: 0.038333285599946976, acc: 0.9791994690895081
iter: 7900, loss_all: 0.007633922155946493, acc: 0.9875993132591248
iter: 8000, loss_all: 0.01156863383948803, acc: 0.9863993525505066
iter: 8100, loss_all: 0.009960826486349106, acc: 0.9871993660926819
iter: 8200, loss_all: 0.027795717120170593, acc: 0.9799994230270386
iter: 8300, loss_all: 0.024801375344395638, acc: 0.9815995097160339
iter: 8400, loss_all: 0.010460734367370605, acc: 0.986799418926239
iter: 8500, loss_all: 0.012654603458940983, acc: 0.9871993660926819
iter: 8600, loss_all: 0.026965053752064705, acc: 0.9823993444442749
iter: 8700, loss_all: 0.01821606233716011, acc: 0.9831992983818054
iter: 8800, loss_all: 0.006180703639984131, acc: 0.9875993728637695
iter: 8900, loss_all: 0.01457714568823576, acc: 0.985999345779419
iter: 9000, loss_all: 0.010251459665596485, acc: 0.9851994514465332
iter: 9100, loss_all: 0.0135458679869771, acc: 0.9871994256973267
iter: 9200, loss_all: 0.01897074468433857, acc: 0.9843993186950684
iter: 9300, loss_all: 0.00723197590559721, acc: 0.9883993864059448
iter: 9400, loss_all: 0.007958689704537392, acc: 0.9879993796348572
iter: 9500, loss_all: 0.006562748458236456, acc: 0.9875994324684143
iter: 9600, loss_all: 0.018274055793881416, acc: 0.9839993715286255
iter: 9700, loss_all: 0.011970837600529194, acc: 0.9855993390083313
iter: 9800, loss_all: 0.008088096976280212, acc: 0.9855993390083313
iter: 9900, loss_all: 0.00735009741038084, acc: 0.9863994121551514
iter: 10000, loss_all: 0.012551629915833473, acc: 0.9863994121551514
iter: 10100, loss_all: 0.015408379025757313, acc: 0.9851993918418884
iter: 10200, loss_all: 0.022912288084626198, acc: 0.9799994230270386
iter: 10300, loss_all: 0.013507477007806301, acc: 0.9855993986129761
iter: 10400, loss_all: 0.019842777401208878, acc: 0.9827994108200073
iter: 10500, loss_all: 0.0195307657122612, acc: 0.9839993715286255
iter: 10600, loss_all: 0.020752638578414917, acc: 0.984799325466156
iter: 10700, loss_all: 0.00952699314802885, acc: 0.9871993660926819
iter: 10800, loss_all: 0.0018317240756005049, acc: 0.989599347114563
iter: 10900, loss_all: 0.018926281481981277, acc: 0.9851993918418884
iter: 11000, loss_all: 0.015530083328485489, acc: 0.9851993322372437
iter: 11100, loss_all: 0.013420864939689636, acc: 0.9871993660926819
iter: 11200, loss_all: 0.011844209395349026, acc: 0.9875993728637695
iter: 11300, loss_all: 0.01761474646627903, acc: 0.9835993051528931
iter: 11400, loss_all: 0.017360374331474304, acc: 0.9835994243621826
iter: 11500, loss_all: 0.010854074731469154, acc: 0.9867993593215942
iter: 11600, loss_all: 0.003952173516154289, acc: 0.9887993335723877
iter: 11700, loss_all: 0.021472731605172157, acc: 0.9843993782997131
iter: 11800, loss_all: 0.013971992768347263, acc: 0.9855993986129761
iter: 11900, loss_all: 0.006706533022224903, acc: 0.9875993728637695
iter: 12000, loss_all: 0.007970241829752922, acc: 0.9875993728637695
iter: 12100, loss_all: 0.012358387000858784, acc: 0.9851993322372437
iter: 12200, loss_all: 0.01243766862899065, acc: 0.9855993986129761
iter: 12300, loss_all: 0.018150299787521362, acc: 0.9855992794036865
iter: 12400, loss_all: 0.012004275806248188, acc: 0.9871993660926819
iter: 12500, loss_all: 0.010312143713235855, acc: 0.9871994256973267
iter: 12600, loss_all: 0.0036141234450042248, acc: 0.9891993403434753
iter: 12700, loss_all: 0.0239186380058527, acc: 0.9823994636535645
iter: 12800, loss_all: 0.01774352230131626, acc: 0.9843993186950684
iter: 12900, loss_all: 0.03539863973855972, acc: 0.9787994027137756
iter: 13000, loss_all: 0.003511232789605856, acc: 0.9887993931770325
iter: 13100, loss_all: 0.005799804348498583, acc: 0.9887993335723877
iter: 13200, loss_all: 0.002567485673353076, acc: 0.9883993864059448
iter: 13300, loss_all: 0.0011988759506493807, acc: 0.9899993538856506
iter: 13400, loss_all: 0.0035623309668153524, acc: 0.9879993796348572
iter: 13500, loss_all: 0.0044531445018947124, acc: 0.9883993268013
iter: 13600, loss_all: 0.018454374745488167, acc: 0.9835993051528931
iter: 13700, loss_all: 0.022602928802371025, acc: 0.9839993715286255
iter: 13800, loss_all: 0.018254343420267105, acc: 0.9835994243621826
iter: 13900, loss_all: 0.013302747160196304, acc: 0.9863993525505066
iter: 14000, loss_all: 0.005460849031805992, acc: 0.9879993200302124
iter: 14100, loss_all: 0.010656275786459446, acc: 0.9871994256973267
iter: 14200, loss_all: 0.016441598534584045, acc: 0.9847993850708008
iter: 14300, loss_all: 0.01255055982619524, acc: 0.9859994053840637
iter: 14400, loss_all: 0.006547634955495596, acc: 0.9887993335723877
iter: 14500, loss_all: 0.01093004085123539, acc: 0.9863994121551514
iter: 14600, loss_all: 0.012384063564240932, acc: 0.9851993918418884
iter: 14700, loss_all: 0.0057061221450567245, acc: 0.9883993864059448
iter: 14800, loss_all: 0.004675843752920628, acc: 0.9887993931770325
iter: 14900, loss_all: 0.004606923088431358, acc: 0.9887993931770325
iter: 15000, loss_all: 0.009088766761124134, acc: 0.986799418926239
